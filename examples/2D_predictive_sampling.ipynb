{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from vpsto.vptraj import VPTraj\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q0 = np.array([-.5, -.5]) # Initial position\n",
    "qg = np.array([.5, .5]) # Goal position\n",
    "dq0 = np.array([0, 0]) # Initial velocity\n",
    "dqg = np.array([0, 0]) # Goal velocity\n",
    "\n",
    "bounds = 0.8*np.array([[-1, 1], [-1, 1]]) # Bounds on position\n",
    "\n",
    "R = 1e1 # Penalty on control effort (acceleration)\n",
    "Q_min = 1e0 # Minimum penalty on control error (position)\n",
    "Q_max = 1e3 # Maximum penalty on control error (position)\n",
    "factor_Q_min = 1e-1\n",
    "factor_Q_max = 1e1\n",
    "\n",
    "N_via = 4 # Number of via points\n",
    "N_candidates = 1000 # Number of candidates to sample\n",
    "N_eval = 25 # Number of pos,vel,acc samples to evaluate along each candidate trajectory\n",
    "\n",
    "vel_lim = 0.2 # Velocity limit (m/s in each dimension)\n",
    "acc_lim = 1 # Acceleration limit (m/s^2 in each dimension)\n",
    "\n",
    "num_obstacles = 10 # Number of obstacles\n",
    "robot_radius = 0.1 # Radius of robot (m)\n",
    "obstacle_radius = 0.1 # Radius of obstacle (m)\n",
    "\n",
    "dt_control = 0.05 # Time step for control (s)\n",
    "sim_duration = 30 # Duration of simulation (s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Obstacle:\n",
    "    def __init__(self, pos, vel, radius, robot_radius):\n",
    "        self.pos = pos # initial position\n",
    "        self.vel = vel # initial velocity\n",
    "        self.radius = radius # radius of obstacle\n",
    "        self.robot_radius = robot_radius # radius of robot\n",
    "        self.d_sq = (radius + robot_radius + 0.01)**2 # distance squared for faster collision checking\n",
    "\n",
    "        self.history = [self.pos.copy()] # history of obstacle positions\n",
    "\n",
    "    def reset(self):\n",
    "        self.history = [self.pos.copy()]\n",
    "\n",
    "    def step(self, dt):\n",
    "        # Check if obstacle would bounce off a wall and reverse velocity if so\n",
    "        for i in range(2):\n",
    "            if self.pos[i] + self.vel[i] * dt < bounds[i, 0] + self.radius:\n",
    "                self.vel[i] = -self.vel[i]\n",
    "            if self.pos[i] + self.vel[i] * dt > bounds[i, 1] - self.radius:\n",
    "                self.vel[i] = -self.vel[i]\n",
    "        self.pos += self.vel * dt\n",
    "        self.history.append(self.pos.copy())\n",
    "\n",
    "    def is_collision(self, robot_pos):\n",
    "        # Check if the robot at pos is in collision with the obstacle\n",
    "        # pos is a (N+1)D array of shape (M1, ..., MN, 2)\n",
    "        # Returns a ND array of shape (M1, ..., MN) containing True if there is a collision\n",
    "        return np.sum((robot_pos - self.pos)**2, axis=-1) < self.d_sq\n",
    "    \n",
    "    def predict_collision(self, robot_pos_traj, T):\n",
    "        # Predict if there will be a collision in the next T seconds\n",
    "        # pos_traj is a 2D array of shape (N, 2) containing the discretized trajectory of the robot\n",
    "        # T is a scalar\n",
    "        # Returns the number of collisions in the next T seconds\n",
    "        N = robot_pos_traj.shape[0]\n",
    "        t_traj = np.linspace(0, T, N)\n",
    "        obs_traj = self.pos + self.vel * t_traj[:, None]\n",
    "        return np.sum((robot_pos_traj - obs_traj)**2, axis=-1) < self.d_sq\n",
    "    \n",
    "    def predict_collision_batch(self, robot_pos_traj, T):\n",
    "        # Predict if there will be a collision in the next T seconds\n",
    "        # pos_traj is a 3D array of shape (N_batch, N, 2) containing the discretized trajectories of the robot\n",
    "        # T is a vector of shape (N_batch,)\n",
    "        # Returns the number of collisions in the next T seconds for each batch element\n",
    "        N_batch, N, _ = robot_pos_traj.shape\n",
    "        t_traj = np.linspace(0, T, N)\n",
    "        obs_traj = self.pos + np.swapaxes(self.vel * t_traj[:, :, None], 0, 1) # (N_batch, N, 2)\n",
    "        return np.sum((robot_pos_traj - obs_traj)**2, axis=-1) < self.d_sq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictiveSamplingController:\n",
    "    def __init__(self, N_eval, N_via, vel_lim, acc_lim, bounds, qg, obstacles, dt_control, N_candidates, R):\n",
    "        self.vptraj = VPTraj(ndof=2, N_eval=N_eval, N_via=N_via, vel_lim=vel_lim, acc_lim=acc_lim)\n",
    "        self.vptraj_idle = VPTraj(ndof=2, N_eval=N_eval, N_via=1, vel_lim=vel_lim, acc_lim=acc_lim)\n",
    "\n",
    "        self.bounds = bounds # Bounds on position\n",
    "        self.qg = qg # Goal position\n",
    "        self.obstacles = obstacles # List of obstacles to avoid\n",
    "        self.dt_control = dt_control # Time step for control\n",
    "        self.N_candidates = N_candidates # Number of candidate trajectories to sample\n",
    "        self.R = R # Acceleration penalty at sampling stage (not considered in loss function)\n",
    "        self.Q = Q_max # Bias towards trajectories that go closer to the goal (updated at each iteration)\n",
    "        self.acc_lim = acc_lim # Acceleration limit (used for constructing idle trajectory)\n",
    "\n",
    "        self.p_next = None # Candidate trajectory parameter for next iteration\n",
    "        self.T_next = None # Candidate trajectory duration for next iteration\n",
    "        self.samples_log = [] # Log of all candidate trajectories\n",
    "        self.samples_loss_log = [] # Log of all candidate trajectories' losses\n",
    "        self.sol_log = [] # Log of all solutions\n",
    "        self.Q_log = [] # Log of all Q values\n",
    "\n",
    "    def reset(self):\n",
    "        self.p_next = None\n",
    "        self.T_next = None\n",
    "        self.samples_log = []\n",
    "        self.samples_loss_log = []\n",
    "        self.sol_log = []\n",
    "        self.Q_log = []\n",
    "\n",
    "    # Loss function for the candidate trajectories\n",
    "    def loss_fn(self, q, dq, ddq, T):\n",
    "        # Penalize trajectory duration\n",
    "        duration_cost = T\n",
    "        # Penalize control error\n",
    "        qT = q[:,-1] # Final position of all candidates\n",
    "        terminal_cost = 1e3 * np.sum((qT - self.qg)**2, axis=1) # Squared error\n",
    "        # Penalize position limit violations (soft constraint)\n",
    "        num_violations = np.sum((q < self.bounds[:,0] + robot_radius) | (q > self.bounds[:,1] - robot_radius), axis=(1,2))\n",
    "        limit_violation_cost = 1e6 * num_violations\n",
    "        # Penalize collisions with the obstacles (soft constraint)\n",
    "        collision_cost = 0\n",
    "        for obs in self.obstacles:\n",
    "            collision_cost += 1e6 * np.sum(obs.predict_collision_batch(q, T), axis=1)\n",
    "\n",
    "        return terminal_cost + limit_violation_cost + duration_cost + collision_cost\n",
    "    \n",
    "    # Control function that samples candidate trajectories and chooses the best one\n",
    "    def predictive_sampling(self, q, dq):\n",
    "        # Compute how many constraint violations have occurred in the previous iteration\n",
    "        if len(self.samples_loss_log) > 0:\n",
    "            num_violations = np.sum(self.samples_loss_log[-1] > 1e6)\n",
    "        else:\n",
    "            num_violations = 0\n",
    "        # Compute how strong the bias should be towards the goal:\n",
    "        # - If there have been few constraint violations, increase the bias\n",
    "        # - If there have been many constraint violations, decrease the bias\n",
    "        self.Q *= np.clip(np.exp(-3 * (num_violations/self.N_candidates-0.5)), factor_Q_min, factor_Q_max)\n",
    "        self.Q = np.clip(self.Q, Q_min, Q_max)\n",
    "        self.Q_log.append(self.Q)\n",
    "\n",
    "        # Sample candidate trajectories, compute their loss and return the best one\n",
    "        pos, vel, acc, p, T = self.vptraj.sample_trajectories(self.N_candidates, q, dq0=dq, qT=self.qg, \n",
    "                                                              dqT=np.zeros_like(dq), Q=self.Q, R=self.R)\n",
    "        loss = self.loss_fn(pos[:,1:], vel[:,1:], acc[:,1:], T)\n",
    "        self.samples_log.append(pos)\n",
    "        self.samples_loss_log.append(loss)\n",
    "        i_best = np.argmin(loss)\n",
    "        return p[i_best], loss[i_best], T[i_best]\n",
    "    \n",
    "    # Control function that reuses the previous solution\n",
    "    def previous_sol(self, q, dq):\n",
    "        if self.p_next is None:\n",
    "            return None, np.inf, 0\n",
    "        # Compute trajectory with previous solution\n",
    "        pos, vel, acc = self.vptraj.get_trajectory(self.p_next, q, dq0=dq, dqT=np.zeros_like(dq), T=self.T_next)\n",
    "        loss = self.loss_fn(pos[:,1:], vel[:,1:], acc[:,1:], [self.T_next])[0]\n",
    "        return self.p_next, loss, self.T_next\n",
    "    \n",
    "    # Control function that makes the robot come to stop as fast as possible\n",
    "    def idle(self, q, dq):\n",
    "        # Assuming constant acceleration\n",
    "        T_idle = np.max(np.abs(dq) / self.acc_lim)\n",
    "        # Compute the stopping position\n",
    "        q_idle = q + 0.5 * dq * T_idle\n",
    "        pos, vel, acc = self.vptraj_idle.get_trajectory(q_idle, q, dq0=dq, dqT=np.zeros_like(dq), T=T_idle)\n",
    "        loss = self.loss_fn(pos[:,1:], vel[:,1:], acc[:,1:], [T_idle])[0]\n",
    "        # Check obstacle collision also for two seconds ahead\n",
    "        q_list = np.vstack((q, q_idle))\n",
    "        loss += 1e6 * np.sum([obs.predict_collision(q_list, T_idle + 2) for obs in self.obstacles])\n",
    "        t_next = np.min([T_idle, self.dt_control])\n",
    "        if t_next < dt_control:\n",
    "            return q_idle, np.zeros_like(dq), loss, 0\n",
    "        q_next, dq_next, _ = self.vptraj_idle.get_trajectory_at_time(t_next, q_idle, q, dq0=dq, dqT=np.zeros_like(dq), T=T_idle)\n",
    "        return q_next.squeeze(), dq_next.squeeze(), loss, T_idle\n",
    "    \n",
    "    def control(self, q, dq):\n",
    "        # Compute idle trajectory\n",
    "        q_next_idle, dq_next_idle, loss_idle, T_idle = self.idle(q, dq)\n",
    "        # Compute trajectory with previous solution\n",
    "        p_prev, loss_prev, T_prev = self.previous_sol(q, dq)\n",
    "        # Compute trajectory with predictive sampling\n",
    "        p_samp, loss_samp, T_samp = self.predictive_sampling(q, dq)\n",
    "\n",
    "        # Choose the best trajectory\n",
    "        if loss_idle < loss_prev and loss_idle < loss_samp:\n",
    "            print(\"Idle:\", loss_idle, end='\\r')\n",
    "            self.p_next = None\n",
    "            self.T_next = None\n",
    "            loss_best = loss_idle\n",
    "            self.sol_log.append(np.vstack((q, q_next_idle)))\n",
    "            return q_next_idle, dq_next_idle\n",
    "        elif loss_prev < loss_samp:\n",
    "            print(\"Previous:\", loss_prev, end='\\r')\n",
    "            p_best = p_prev\n",
    "            T_best = T_prev\n",
    "            loss_best = loss_prev\n",
    "        else:\n",
    "            print(\"Sampling:\", loss_samp, end='\\r')\n",
    "            p_best = p_samp\n",
    "            T_best = T_samp\n",
    "            loss_best = loss_samp\n",
    "        if T_best < self.dt_control:\n",
    "            self.p_next = None\n",
    "            self.T_next = None\n",
    "            self.sol_log.append(np.vstack((q, q)))\n",
    "            return p_best[-self.vptraj.ndof:], np.zeros_like(dq)\n",
    "        # Compute trajectory parameter and duration for next time step\n",
    "        self.T_next = T_best - self.dt_control\n",
    "        t_next = np.linspace(0, self.T_next, self.vptraj.N_via+1) + self.dt_control\n",
    "        q_next, dq_next, _ = self.vptraj.get_trajectory_at_time(t_next, p_best, q, dq0=dq, \n",
    "                                                                dqT=np.zeros_like(dq), T=T_best)\n",
    "        self.p_next = q_next[1:].flatten()\n",
    "        self.sol_log.append(q_next)\n",
    "\n",
    "        return q_next[0], dq_next[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize obstacles and controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize obstacles at random positions and velocities\n",
    "obstacles = []\n",
    "for i in range(num_obstacles):\n",
    "    valid = False\n",
    "    while not valid:\n",
    "        pos = np.random.uniform(bounds[:, 0] + obstacle_radius, bounds[:, 1] - obstacle_radius)\n",
    "        vel = np.random.uniform(-.1, .1, size=2)\n",
    "        obs = Obstacle(pos, vel, obstacle_radius, robot_radius)\n",
    "        valid = not obs.is_collision(q0)\n",
    "    obstacles.append(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controller = PredictiveSamplingController(N_eval, N_via, vel_lim, acc_lim, bounds, qg, obstacles, dt_control, N_candidates, R)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate closed-loop system for *sim_duration* seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_sim = [q0]\n",
    "dq_sim = [dq0]\n",
    "\n",
    "I = int(sim_duration / dt_control)\n",
    "for i in range(I):\n",
    "    for obs in obstacles:\n",
    "        obs.step(dt_control) # Simulate obstacle motion\n",
    "    q, dq = controller.control(q_sim[-1], dq_sim[-1]) # Compute next desired position and velocity\n",
    "    # As we pretend we have a perfect low-level controller, we can directly apply the desired state\n",
    "    q_sim.append(q.copy()) \n",
    "    dq_sim.append(dq.copy())\n",
    "\n",
    "q_sim = np.array(q_sim)[1:]\n",
    "dq_sim = np.array(dq_sim)[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(np.linspace(0, sim_duration, len(q_sim)), q_sim)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(np.linspace(0, sim_duration, len(dq_sim)), dq_sim)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(np.linspace(0, sim_duration, len(controller.Q_log)), np.log10(controller.Q_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps = 30\n",
    "\n",
    "# Animate the simulation\n",
    "fig = plt.figure(figsize=(10,10), dpi=100)\n",
    "ax = plt.gca()\n",
    "ax.set_facecolor('k')\n",
    "ax.set_xlim(bounds[0])\n",
    "ax.set_ylim(bounds[1])\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "\n",
    "obstacle_paches = []\n",
    "for obs in obstacles:\n",
    "    obstacle_paches.append(plt.Circle(obs.pos, obs.radius, color='gray', alpha=.5))\n",
    "    ax.add_patch(obstacle_paches[-1])\n",
    "\n",
    "robot_patch = plt.Circle(q0, robot_radius, color='w', label='Robot')\n",
    "ax.add_patch(robot_patch)\n",
    "plt.scatter(qg[0], qg[1], c='w', marker='x', s=100, label='Goal')\n",
    "\n",
    "pred_line, = ax.plot([], [], 'm', lw=2.5, label='Simulated trajectory', zorder=10)\n",
    "time_text = ax.text(0.02, 0.95, '', transform=ax.transAxes, color='w', fontsize=14)\n",
    "\n",
    "# create a plot for each sample in samples_log\n",
    "sample_lines = []\n",
    "for i in range(N_candidates):\n",
    "    line, = ax.plot([], [], 'g', alpha=.25)\n",
    "    sample_lines.append(line)\n",
    "\n",
    "def init():\n",
    "    pred_line.set_data([], [])\n",
    "    time_text.set_text('')\n",
    "    return pred_line, time_text\n",
    "\n",
    "def animate(i_):\n",
    "    i = np.min([len(q_sim)-1, int(i_ / (dt_control * fps))])\n",
    "    pred_line.set_data(controller.sol_log[i][:,0], controller.sol_log[i][:,1])\n",
    "    robot_patch.center = q_sim[i]\n",
    "    print(int(i/len(q_sim)*100), end='%\\r')\n",
    "    for j, obs in enumerate(obstacles):\n",
    "        obstacle_paches[j].center = obs.history[i]\n",
    "    loss = controller.samples_loss_log[i]\n",
    "    rewards = 1 - (loss - np.min(loss)) / (np.max(loss) - np.min(loss))\n",
    "    for j in range(N_candidates):\n",
    "        sample_lines[j].set_data(controller.samples_log[i][j,:,0], controller.samples_log[i][j,:,1])\n",
    "        sample_lines[j].set_color((1-rewards[j], rewards[j], 0))\n",
    "    time_text.set_text('time = %.1f' % (i*dt_control))\n",
    "    return pred_line, time_text\n",
    "\n",
    "anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
    "                               frames=int(len(q_sim) * dt_control * fps), interval=1e3/fps, blit=True)\n",
    "\n",
    "anim.save('animation.mp4', fps=fps, codec='libx264')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
